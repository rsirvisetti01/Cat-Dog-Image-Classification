{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This cell downloads the dataset from kaggle and imports important libraries for our code to run"
      ],
      "metadata": {
        "id": "DD5Qs0PbAkxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N94zeOWmm70v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90350bc-3ed2-4d6b-c604-eba7587d02a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset\n",
            "License(s): other\n",
            "Downloading microsoft-catsvsdogs-dataset.zip to /content\n",
            " 98% 772M/788M [00:03<00:00, 264MB/s]\n",
            "100% 788M/788M [00:03<00:00, 265MB/s]\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset\n",
        "!pip install split-folders\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import splitfolders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell extracts the zip file from the kaggle dataset and puts it in the \"content\" directory in a folder called \"PetImages.\" Splitfolders creates a new folder called \"output.\" Splitfolders takes the \"PetImages\" directory and splits it into 3 new subdirectories for training, validation, and testing based on the ratio parameter and puts these new subdirectories in the \"output\" folder.    "
      ],
      "metadata": {
        "id": "y9aTbcKKAvod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Qi5-fOfs0HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270a42d3-aaae-452e-e5bf-0f5884de2a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 25002 files [00:03, 6844.95 files/s]\n"
          ]
        }
      ],
      "source": [
        "local_zip = '/content/microsoft-catsvsdogs-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "splitfolders.ratio('/content/PetImages/', output=\"output\", seed=1337, ratio=(0.7, 0.1, 0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates the file paths for our cat and dog training, validation, and testing directories"
      ],
      "metadata": {
        "id": "Cb5VJLO-OKAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f89eDm3GwpFF"
      },
      "outputs": [],
      "source": [
        "base_dir_train = '/content/output/train'\n",
        "base_dir_val = '/content/output/val'\n",
        "base_dir_test = '/content/output/test'\n",
        "\n",
        "os.listdir(base_dir_train)\n",
        "os.listdir(base_dir_val)\n",
        "os.listdir(base_dir_test)\n",
        "\n",
        "cat_train_dir = os.path.join(base_dir_train, \"Cat\")\n",
        "dog_train_dir = os.path.join(base_dir_train, \"Dog\")\n",
        "\n",
        "cat_val_dir = os.path.join(base_dir_val, \"Cat\")\n",
        "dog_val_dir = os.path.join(base_dir_val, \"Dog\")\n",
        "\n",
        "cat_test_dir = os.path.join(base_dir_test, \"Cat\")\n",
        "dog_test_dir = os.path.join(base_dir_test, \"Dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a folder path as an input and removes any images in the folder that cannot be loaded to prevent issues when training or testing the model"
      ],
      "metadata": {
        "id": "wx2T30LKOTBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def check_images(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add other image extensions if needed\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                img = Image.open(filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {filepath}: {e}\")\n",
        "                os.remove(filepath) # Remove the problematic file"
      ],
      "metadata": {
        "id": "OUYyMQZ7m6pV",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check all of our directories with the function above and remove any problematic images"
      ],
      "metadata": {
        "id": "61QWicZWOg9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_images(cat_train_dir)\n",
        "check_images(dog_train_dir)\n",
        "\n",
        "check_images(cat_val_dir)\n",
        "check_images(dog_val_dir)\n",
        "\n",
        "check_images(cat_test_dir)\n",
        "check_images(dog_test_dir)"
      ],
      "metadata": {
        "id": "dAy497feVrN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9ef46a-3067-4591-a4ed-6742f96b2db9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/output/train/Cat/666.jpg: cannot identify image file '/content/output/train/Cat/666.jpg'\n",
            "Error loading image /content/output/train/Dog/11702.jpg: cannot identify image file '/content/output/train/Dog/11702.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create our train and validation data generator here. It takes the training directory path, augments the data based on the parameters we set, and labels each image based on the name of the directory it came from. In our case, each image would be labeled a \"Cat\" or \"Dog\" based on the directory it came from."
      ],
      "metadata": {
        "id": "HqtprNM766aJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EGoKxryOlQDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911150b3-1590-436f-8791-4d5b8395f617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17496 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "img_width = 150\n",
        "img_height = 150\n",
        "batch_size = 40\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    rotation_range=40,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir_train,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    base_dir_val,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create our convolutional neural network with 3 hidden layers and MaxPooling. We also have an output layer that uses a sigmoid function because we are doing a binary classification."
      ],
      "metadata": {
        "id": "uMRjErPNBO1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0d4uSKH8Louy"
      },
      "outputs": [],
      "source": [
        "img_input = layers.Input(shape=(img_width, img_height, 3))\n",
        "\n",
        "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(img_input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize the model"
      ],
      "metadata": {
        "id": "o60rKNZNBT_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aykJhQkhTOeD",
        "outputId": "d97cc63b-6974-4bab-dd06-83f5c8d86210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 74, 74, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18496)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               9470464   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9494561 (36.22 MB)\n",
            "Trainable params: 9494561 (36.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now compile our model with the \"adam\" optimizer and train it using our train generator over 30 epochs. We also validate it using our validation generator."
      ],
      "metadata": {
        "id": "9pS7TinwBW1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWJd4MmobkrT",
        "outputId": "129305b0-f80e-44c8-e22b-4770a814d5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "438/438 [==============================] - 124s 279ms/step - loss: 0.6698 - accuracy: 0.5854 - val_loss: 0.6075 - val_accuracy: 0.6536\n",
            "Epoch 2/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.6027 - accuracy: 0.6714 - val_loss: 0.5745 - val_accuracy: 0.6976\n",
            "Epoch 3/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.5655 - accuracy: 0.7047 - val_loss: 0.5161 - val_accuracy: 0.7416\n",
            "Epoch 4/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.5331 - accuracy: 0.7311 - val_loss: 0.4802 - val_accuracy: 0.7820\n",
            "Epoch 5/30\n",
            "438/438 [==============================] - 122s 277ms/step - loss: 0.5183 - accuracy: 0.7429 - val_loss: 0.5071 - val_accuracy: 0.7456\n",
            "Epoch 6/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.4955 - accuracy: 0.7546 - val_loss: 0.4543 - val_accuracy: 0.7956\n",
            "Epoch 7/30\n",
            "438/438 [==============================] - 121s 275ms/step - loss: 0.4912 - accuracy: 0.7613 - val_loss: 0.4651 - val_accuracy: 0.7856\n",
            "Epoch 8/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.4782 - accuracy: 0.7691 - val_loss: 0.4332 - val_accuracy: 0.8112\n",
            "Epoch 9/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.4636 - accuracy: 0.7805 - val_loss: 0.4057 - val_accuracy: 0.8204\n",
            "Epoch 10/30\n",
            "438/438 [==============================] - 121s 275ms/step - loss: 0.4459 - accuracy: 0.7942 - val_loss: 0.4282 - val_accuracy: 0.8080\n",
            "Epoch 11/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.4471 - accuracy: 0.7894 - val_loss: 0.3739 - val_accuracy: 0.8368\n",
            "Epoch 12/30\n",
            "438/438 [==============================] - 121s 277ms/step - loss: 0.4365 - accuracy: 0.7983 - val_loss: 0.3902 - val_accuracy: 0.8264\n",
            "Epoch 13/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.4278 - accuracy: 0.8060 - val_loss: 0.3920 - val_accuracy: 0.8316\n",
            "Epoch 14/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.4209 - accuracy: 0.8064 - val_loss: 0.4057 - val_accuracy: 0.8208\n",
            "Epoch 15/30\n",
            "438/438 [==============================] - 119s 273ms/step - loss: 0.4061 - accuracy: 0.8130 - val_loss: 0.3995 - val_accuracy: 0.8228\n",
            "Epoch 16/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.3983 - accuracy: 0.8194 - val_loss: 0.3365 - val_accuracy: 0.8536\n",
            "Epoch 17/30\n",
            "438/438 [==============================] - 120s 273ms/step - loss: 0.3878 - accuracy: 0.8224 - val_loss: 0.3803 - val_accuracy: 0.8324\n",
            "Epoch 18/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.3861 - accuracy: 0.8278 - val_loss: 0.3593 - val_accuracy: 0.8472\n",
            "Epoch 19/30\n",
            "438/438 [==============================] - 120s 275ms/step - loss: 0.3840 - accuracy: 0.8261 - val_loss: 0.3388 - val_accuracy: 0.8496\n",
            "Epoch 20/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.3741 - accuracy: 0.8317 - val_loss: 0.4298 - val_accuracy: 0.8028\n",
            "Epoch 21/30\n",
            "438/438 [==============================] - 120s 275ms/step - loss: 0.3710 - accuracy: 0.8310 - val_loss: 0.3288 - val_accuracy: 0.8656\n",
            "Epoch 22/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.3665 - accuracy: 0.8362 - val_loss: 0.3512 - val_accuracy: 0.8516\n",
            "Epoch 23/30\n",
            "438/438 [==============================] - 119s 272ms/step - loss: 0.3545 - accuracy: 0.8424 - val_loss: 0.3013 - val_accuracy: 0.8748\n",
            "Epoch 24/30\n",
            "438/438 [==============================] - 121s 275ms/step - loss: 0.3484 - accuracy: 0.8496 - val_loss: 0.3127 - val_accuracy: 0.8692\n",
            "Epoch 25/30\n",
            "438/438 [==============================] - 120s 275ms/step - loss: 0.3461 - accuracy: 0.8440 - val_loss: 0.3386 - val_accuracy: 0.8592\n",
            "Epoch 26/30\n",
            "438/438 [==============================] - 120s 273ms/step - loss: 0.3398 - accuracy: 0.8523 - val_loss: 0.2899 - val_accuracy: 0.8792\n",
            "Epoch 27/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.3350 - accuracy: 0.8557 - val_loss: 0.2811 - val_accuracy: 0.8868\n",
            "Epoch 28/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.3270 - accuracy: 0.8544 - val_loss: 0.2801 - val_accuracy: 0.8908\n",
            "Epoch 29/30\n",
            "438/438 [==============================] - 121s 276ms/step - loss: 0.3235 - accuracy: 0.8596 - val_loss: 0.3497 - val_accuracy: 0.8448\n",
            "Epoch 30/30\n",
            "438/438 [==============================] - 120s 274ms/step - loss: 0.3253 - accuracy: 0.8577 - val_loss: 0.2867 - val_accuracy: 0.8812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7910b4a33f70>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=30, validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we achieved an accuracy of around 85.8%. To increase accuracy, we can increase the amount of training images used by changing the ratio parameter in splitfolders. However, this comes with the tradeoff of taking more time to train our model. For example, if we use 7,500 images to train, our model will take about half the time to train, and we will only lose about 3% accuracy. Our validation accuracy is 88.1%, which suggests that we are not overfitting."
      ],
      "metadata": {
        "id": "RagNMUmiBiJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we create a function to process images before we try to classify them with our model"
      ],
      "metadata": {
        "id": "PvbTaYPGByGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wINLUyjMgUFU"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "  img = image.load_img(img_path, target_size=(img_width, img_height))  # Load image and resize\n",
        "  img = image.img_to_array(img)  # Convert to numpy array\n",
        "  img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "  img /= 255.0  # Normalize pixel values\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we try classifying one our pictures from the \"Cat\" directory and we see that our model correctly identifies it as a Cat."
      ],
      "metadata": {
        "id": "2mkGZ6xVB4nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/PetImages/Cat/10040.jpg'\n",
        "preprocessed_image = preprocess_image(image_path)\n",
        "prediction = model.predict(preprocessed_image)\n",
        "\n",
        "if prediction[0][0] < 0.5:\n",
        "  print('Predicted class: Cat')\n",
        "else:\n",
        "  print('Predicted class: Dog')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYViyAG4eWwL",
        "outputId": "e2ff2979-442d-433c-a9a1-0e179627ad5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "Predicted class: Cat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}