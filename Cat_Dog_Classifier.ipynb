{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This cell downloads the dataset from kaggle and imports important libraries for our code to run"
      ],
      "metadata": {
        "id": "DD5Qs0PbAkxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N94zeOWmm70v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bbac63-617c-4adc-b306-3eaad88785ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset\n",
            "License(s): other\n",
            "Downloading microsoft-catsvsdogs-dataset.zip to /content\n",
            "100% 787M/788M [00:04<00:00, 207MB/s]\n",
            "100% 788M/788M [00:04<00:00, 202MB/s]\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset\n",
        "!pip install split-folders\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import splitfolders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell extracts the zip file from the kaggle dataset and puts it in the \"content\" directory in a folder called \"PetImages.\" Splitfolders creates a new folder called \"output.\" Splitfolders takes the \"PetImages\" directory and splits it into 3 new subdirectories for training, validation, and testing based on the ratio parameter and puts these new subdirectories in the \"output\" folder.    "
      ],
      "metadata": {
        "id": "y9aTbcKKAvod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qi5-fOfs0HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164ac0af-9ef6-44c7-89c6-74ab17252c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 25002 files [00:03, 6492.84 files/s]\n"
          ]
        }
      ],
      "source": [
        "local_zip = '/content/microsoft-catsvsdogs-dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "splitfolders.ratio('/content/PetImages/', output=\"output\", seed=1337, ratio=(0.3, 0.1, 0.6))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates the file paths for our cat and dog training, validation, and testing directories"
      ],
      "metadata": {
        "id": "Cb5VJLO-OKAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f89eDm3GwpFF"
      },
      "outputs": [],
      "source": [
        "base_dir_train = '/content/output/train'\n",
        "base_dir_val = '/content/output/val'\n",
        "base_dir_test = '/content/output/test'\n",
        "\n",
        "os.listdir(base_dir_train)\n",
        "os.listdir(base_dir_val)\n",
        "os.listdir(base_dir_test)\n",
        "\n",
        "cat_train_dir = os.path.join(base_dir_train, \"Cat\")\n",
        "dog_train_dir = os.path.join(base_dir_train, \"Dog\")\n",
        "\n",
        "cat_val_dir = os.path.join(base_dir_val, \"Cat\")\n",
        "dog_val_dir = os.path.join(base_dir_val, \"Dog\")\n",
        "\n",
        "cat_test_dir = os.path.join(base_dir_test, \"Cat\")\n",
        "dog_test_dir = os.path.join(base_dir_test, \"Dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a folder path as an input and removes any images in the folder that cannot be loaded to prevent issues when training or testing the model"
      ],
      "metadata": {
        "id": "wx2T30LKOTBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def check_images(folder_path):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add other image extensions if needed\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                img = Image.open(filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {filepath}: {e}\")\n",
        "                os.remove(filepath) # Remove the problematic file"
      ],
      "metadata": {
        "id": "OUYyMQZ7m6pV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check all of our directories with the function above and remove any problematic images"
      ],
      "metadata": {
        "id": "61QWicZWOg9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_images(cat_train_dir)\n",
        "check_images(dog_train_dir)\n",
        "\n",
        "check_images(cat_val_dir)\n",
        "check_images(dog_val_dir)\n",
        "\n",
        "check_images(cat_test_dir)\n",
        "check_images(dog_test_dir)"
      ],
      "metadata": {
        "id": "dAy497feVrN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfcf685-c6e5-47b1-91b2-7fa8aa63c130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/output/train/Cat/666.jpg: cannot identify image file '/content/output/train/Cat/666.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/output/train/Dog/11702.jpg: cannot identify image file '/content/output/train/Dog/11702.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create our train and validation data generator here. It takes the training directory path, augments the data based on the parameters we set, and labels each image based on the name of the directory it came from. In our case, each image would be labeled a \"Cat\" or \"Dog\" based on the directory it came from."
      ],
      "metadata": {
        "id": "HqtprNM766aJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGoKxryOlQDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eacab4d-8073-4fac-f03a-9db1f1e17987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7496 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "img_width = 150\n",
        "img_height = 150\n",
        "batch_size = 40\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    rotation_range=40,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir_train,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    base_dir_val,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create our convolutional neural network with 3 hidden layers and MaxPooling. We also have an output layer that uses a sigmoid function because we are doing a binary classification."
      ],
      "metadata": {
        "id": "uMRjErPNBO1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d4uSKH8Louy"
      },
      "outputs": [],
      "source": [
        "img_input = layers.Input(shape=(img_width, img_height, 3))\n",
        "\n",
        "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(img_input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize the model"
      ],
      "metadata": {
        "id": "o60rKNZNBT_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aykJhQkhTOeD",
        "outputId": "7772f05a-0574-491e-b43e-3af0b37b731b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 74, 74, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18496)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               9470464   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9494561 (36.22 MB)\n",
            "Trainable params: 9494561 (36.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now compile our model with the \"adam\" optimizer and train it using our train generator over 30 epochs. We also validate it using our validation generator."
      ],
      "metadata": {
        "id": "9pS7TinwBW1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWJd4MmobkrT",
        "outputId": "277e87c3-07cc-4ad8-e705-626d6d169c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "188/188 [==============================] - 57s 293ms/step - loss: 0.7017 - accuracy: 0.5510 - val_loss: 0.6614 - val_accuracy: 0.5864\n",
            "Epoch 2/30\n",
            "188/188 [==============================] - 55s 294ms/step - loss: 0.6536 - accuracy: 0.6119 - val_loss: 0.6022 - val_accuracy: 0.6660\n",
            "Epoch 3/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.6292 - accuracy: 0.6434 - val_loss: 0.5859 - val_accuracy: 0.7084\n",
            "Epoch 4/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.6006 - accuracy: 0.6672 - val_loss: 0.5707 - val_accuracy: 0.6928\n",
            "Epoch 5/30\n",
            "188/188 [==============================] - 55s 293ms/step - loss: 0.5843 - accuracy: 0.6901 - val_loss: 0.5415 - val_accuracy: 0.7248\n",
            "Epoch 6/30\n",
            "188/188 [==============================] - 55s 294ms/step - loss: 0.5684 - accuracy: 0.7045 - val_loss: 0.4900 - val_accuracy: 0.7636\n",
            "Epoch 7/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.5558 - accuracy: 0.7136 - val_loss: 0.4750 - val_accuracy: 0.7740\n",
            "Epoch 8/30\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.5320 - accuracy: 0.7343 - val_loss: 0.4896 - val_accuracy: 0.7668\n",
            "Epoch 9/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.5318 - accuracy: 0.7295 - val_loss: 0.5220 - val_accuracy: 0.7404\n",
            "Epoch 10/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.5237 - accuracy: 0.7372 - val_loss: 0.5124 - val_accuracy: 0.7424\n",
            "Epoch 11/30\n",
            "188/188 [==============================] - 55s 292ms/step - loss: 0.5102 - accuracy: 0.7485 - val_loss: 0.4429 - val_accuracy: 0.7960\n",
            "Epoch 12/30\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.5088 - accuracy: 0.7508 - val_loss: 0.4328 - val_accuracy: 0.8068\n",
            "Epoch 13/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.4989 - accuracy: 0.7493 - val_loss: 0.5459 - val_accuracy: 0.7196\n",
            "Epoch 14/30\n",
            "188/188 [==============================] - 55s 290ms/step - loss: 0.4895 - accuracy: 0.7653 - val_loss: 0.4342 - val_accuracy: 0.7960\n",
            "Epoch 15/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.4906 - accuracy: 0.7619 - val_loss: 0.4168 - val_accuracy: 0.8100\n",
            "Epoch 16/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.4756 - accuracy: 0.7683 - val_loss: 0.4215 - val_accuracy: 0.8076\n",
            "Epoch 17/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.4743 - accuracy: 0.7693 - val_loss: 0.4718 - val_accuracy: 0.7680\n",
            "Epoch 18/30\n",
            "188/188 [==============================] - 55s 291ms/step - loss: 0.4660 - accuracy: 0.7792 - val_loss: 0.3894 - val_accuracy: 0.8300\n",
            "Epoch 19/30\n",
            "188/188 [==============================] - 55s 291ms/step - loss: 0.4591 - accuracy: 0.7852 - val_loss: 0.4137 - val_accuracy: 0.8128\n",
            "Epoch 20/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.4573 - accuracy: 0.7851 - val_loss: 0.3908 - val_accuracy: 0.8264\n",
            "Epoch 21/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.4543 - accuracy: 0.7826 - val_loss: 0.3910 - val_accuracy: 0.8204\n",
            "Epoch 22/30\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.4449 - accuracy: 0.7942 - val_loss: 0.3650 - val_accuracy: 0.8404\n",
            "Epoch 23/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.4319 - accuracy: 0.8014 - val_loss: 0.4031 - val_accuracy: 0.8156\n",
            "Epoch 24/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.4282 - accuracy: 0.8068 - val_loss: 0.4268 - val_accuracy: 0.8060\n",
            "Epoch 25/30\n",
            "188/188 [==============================] - 54s 287ms/step - loss: 0.4227 - accuracy: 0.8048 - val_loss: 0.4317 - val_accuracy: 0.8020\n",
            "Epoch 26/30\n",
            "188/188 [==============================] - 55s 290ms/step - loss: 0.4218 - accuracy: 0.8058 - val_loss: 0.4369 - val_accuracy: 0.7980\n",
            "Epoch 27/30\n",
            "188/188 [==============================] - 54s 288ms/step - loss: 0.4254 - accuracy: 0.8038 - val_loss: 0.3773 - val_accuracy: 0.8344\n",
            "Epoch 28/30\n",
            "188/188 [==============================] - 54s 289ms/step - loss: 0.4081 - accuracy: 0.8150 - val_loss: 0.3771 - val_accuracy: 0.8348\n",
            "Epoch 29/30\n",
            "188/188 [==============================] - 55s 291ms/step - loss: 0.3989 - accuracy: 0.8198 - val_loss: 0.3510 - val_accuracy: 0.8424\n",
            "Epoch 30/30\n",
            "188/188 [==============================] - 54s 286ms/step - loss: 0.4030 - accuracy: 0.8164 - val_loss: 0.3691 - val_accuracy: 0.8324\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x788900ec9660>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, epochs=30, validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we achieved an accuracy of around 82%. To increase accuracy, we can increase the amount of training images used by changing the ratio parameter in splitfolders. However, this comes with the tradeoff of taking more time to train our model."
      ],
      "metadata": {
        "id": "RagNMUmiBiJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we create a function to process images before we try to classify them with our model"
      ],
      "metadata": {
        "id": "PvbTaYPGByGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wINLUyjMgUFU"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "  img = image.load_img(img_path, target_size=(img_width, img_height))  # Load image and resize\n",
        "  img = image.img_to_array(img)  # Convert to numpy array\n",
        "  img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "  img /= 255.0  # Normalize pixel values\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we try classifying one our pictures from the \"Cat\" directory and we see that our model correctly identifies it as a Cat."
      ],
      "metadata": {
        "id": "2mkGZ6xVB4nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/PetImages/Cat/10040.jpg'\n",
        "preprocessed_image = preprocess_image(image_path)\n",
        "prediction = model.predict(preprocessed_image)\n",
        "\n",
        "if prediction[0][0] < 0.5:\n",
        "  print('Predicted class: Cat')\n",
        "else:\n",
        "  print('Predicted class: Dog')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYViyAG4eWwL",
        "outputId": "05cc7e63-6a2f-4629-85b2-1aee65083634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 104ms/step\n",
            "Predicted class: Cat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}